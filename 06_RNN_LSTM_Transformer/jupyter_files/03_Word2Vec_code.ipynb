{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aed1f2b-7bb2-460f-8ec3-b08607ce0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ============================================================\n",
    "# WORD2VEC SKIP-GRAM MODEL IN PYTORCH\n",
    "# ============================================================\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-gram model: predict context words from target word\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        \n",
    "        # Input embedding (target word)\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Output embedding (context word)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize with small random values\n",
    "        self.target_embeddings.weight.data.uniform_(-0.5/embedding_dim, \n",
    "                                                      0.5/embedding_dim)\n",
    "        self.context_embeddings.weight.data.uniform_(-0.5/embedding_dim, \n",
    "                                                       0.5/embedding_dim)\n",
    "    \n",
    "    def forward(self, target_word, context_word):\n",
    "        # Get embeddings\n",
    "        target_embed = self.target_embeddings(target_word)    # [batch, embed_dim]\n",
    "        context_embed = self.context_embeddings(context_word)  # [batch, embed_dim]\n",
    "        \n",
    "        # Compute dot product (similarity)\n",
    "        score = torch.sum(target_embed * context_embed, dim=1)\n",
    "        \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ad71af-4682-4a43-9e69-4d3161ebddbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 23\n",
      "Sample tokens: ['the', 'cat', 'sat', 'on', 'the', 'mat', 'the', 'dog', 'sat', 'on']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATA PREPARATION\n",
    "# ============================================================\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are animals\",\n",
    "    \"the cat and dog are friends\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokens = []\n",
    "for sentence in corpus:\n",
    "    tokens.extend(sentence.lower().split())\n",
    "\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Sample tokens: {tokens[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2edf85e-ed78-4251-82dd-73f68ab4b874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size: 13\n",
      "Vocabulary: ['the', 'cat', 'sat', 'on', 'dog', 'and', 'are', 'mat', 'log', 'cats', 'dogs', 'animals', 'friends']\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "from collections import Counter\n",
    "word_counts = Counter(tokens)\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}\n",
    "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {list(vocab.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf90c02-1bfc-4bbf-b10b-62832c213f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training pairs: 86\n",
      "Sample pairs (target -> context):\n",
      "  the -> cat\n",
      "  the -> sat\n",
      "  cat -> the\n",
      "  cat -> sat\n",
      "  cat -> on\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CREATE TRAINING PAIRS (Skip-gram)\n",
    "# ============================================================\n",
    "\n",
    "def create_skipgram_dataset(tokens, vocab, window_size=2):\n",
    "    \"\"\"\n",
    "    Create (target, context) pairs\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for i, target_word in enumerate(tokens):\n",
    "        target_idx = vocab[target_word]\n",
    "        \n",
    "        # Get context words within window\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(tokens), i + window_size + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if i != j:  # Don't pair word with itself\n",
    "                context_word = tokens[j]\n",
    "                context_idx = vocab[context_word]\n",
    "                pairs.append((target_idx, context_idx))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "window_size = 2\n",
    "training_pairs = create_skipgram_dataset(tokens, vocab, window_size)\n",
    "\n",
    "print(f\"\\nTraining pairs: {len(training_pairs)}\")\n",
    "print(\"Sample pairs (target -> context):\")\n",
    "for i in range(5):\n",
    "    target_idx, context_idx = training_pairs[i]\n",
    "    print(f\"  {idx_to_word[target_idx]} -> {idx_to_word[context_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551878a3-5fd1-4a41-b66d-cbf04a1561c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|███████████████████████████▊                                                                                             | 23/100 [00:01<00:03, 22.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 236.3641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|█████████████████████████████████████████████████████▏                                                                   | 44/100 [00:01<00:02, 25.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 230.5507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████████████████████████████████████████████████████████████████████████████▋                                          | 65/100 [00:02<00:01, 25.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Loss: 219.7868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████████████████████████████████████████████████████████████████████████████████████████████████▍                    | 83/100 [00:03<00:00, 25.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Loss: 210.4355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 200.8782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAINING WITH NEGATIVE SAMPLING\n",
    "# ============================================================\n",
    "\n",
    "embedding_dim = 50\n",
    "learning_rate = 0.025\n",
    "epochs = 100\n",
    "num_negative_samples = 5\n",
    "\n",
    "model = SkipGramModel(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Word frequency for negative sampling\n",
    "word_freqs = np.array([word_counts[idx_to_word[i]] for i in range(vocab_size)])\n",
    "word_freqs = word_freqs ** 0.75  # Smooth distribution\n",
    "word_freqs = word_freqs / word_freqs.sum()\n",
    "\n",
    "\n",
    "from tldm import tldm\n",
    "\n",
    "for epoch in tldm(range(epochs), desc='Training'):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for target_idx, context_idx in training_pairs:\n",
    "        # Positive pair\n",
    "        target = torch.LongTensor([target_idx]).to(device)\n",
    "        context = torch.LongTensor([context_idx]).to(device)\n",
    "        \n",
    "        # Positive score (should be high)\n",
    "        pos_score = model(target, context)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score))\n",
    "        \n",
    "        # Negative sampling\n",
    "        neg_indices = np.random.choice(vocab_size, \n",
    "                                       size=num_negative_samples, \n",
    "                                       p=word_freqs)\n",
    "        neg_indices = torch.LongTensor(neg_indices).to(device)\n",
    "        \n",
    "        # Negative scores (should be low)\n",
    "        neg_scores = model(target.repeat(num_negative_samples), neg_indices)\n",
    "        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_scores)))\n",
    "        # Example:\n",
    "        # target_idx = 3  (word \"cat\")\n",
    "        # num_negative_samples = 5\n",
    "        # neg_indices = [7, 2, 9, 1, 4]  (random \"wrong\" words)\n",
    "        # target.repeat(num_negative_samples)         ->        [3, 3, 3, 3, 3]\n",
    "        # compare \"cat\" against each negative sample            [7, 2, 9, 1, 4]\n",
    "        # model([3, 3, 3, 3, 3], [7, 2, 9, 1, 4]) \n",
    "        #  -> neg_scores = [0.8, -0.3, 1.2, 0.1, -0.5]\n",
    "        #  - High score (0.8, 1.2) = BAD! \"cat\" is similar to negative sample\n",
    "        #  - Low score (-0.3, -0.5) = GOOD! \"cat\" is dissimilar to negative sample\n",
    "        #\n",
    "        # neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_scores)))\n",
    "        # -> Make neg_scores LOW (we want dissimilarity)\n",
    "        # -neg_scores: [-0.8, 0.3, -1.2, -0.1, 0.5] => Why negate? Because sigmoid(-x) is high when x is low\n",
    "        # sigmoid(x): [0.31, 0.57, 0.23, 0.48, 0.62]\n",
    "        # - If neg_score was high (0.8) → sigmoid(-0.8) = 0.31 (low)\n",
    "        # - If neg_score was low (-0.5) → sigmoid(0.5) = 0.62 (high)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = pos_loss + neg_loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8207e595-9a93-44cb-9627-aa1e430012b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINED EMBEDDINGS\n",
      "============================================================\n",
      "Shape: (13, 50)\n",
      "\n",
      "Word 'cat' embedding (first 10 dims):\n",
      "[ 0.01814973 -0.5085046   0.14865027  0.1861113  -0.13569748  0.04037447\n",
      " -0.07867606  0.00525767 -0.3990992   0.18380988]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EXTRACT TRAINED EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "embeddings = model.target_embeddings.weight.data.cpu().numpy()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINED EMBEDDINGS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"\\nWord 'cat' embedding (first 10 dims):\")\n",
    "print(embeddings[vocab['cat']][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30043e58-e133-485b-b10c-e9c93be36d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WORD SIMILARITIES\n",
      "============================================================\n",
      "\n",
      "Most similar to 'cat':\n",
      "  mat: 0.7801\n",
      "  log: 0.7175\n",
      "  sat: 0.7101\n",
      "\n",
      "Most similar to 'dog':\n",
      "  the: 0.8487\n",
      "  dogs: 0.6186\n",
      "  log: 0.5724\n",
      "\n",
      "Most similar to 'sat':\n",
      "  mat: 0.9713\n",
      "  cat: 0.7101\n",
      "  on: 0.6464\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COMPUTE SIMILARITIES\n",
    "# ============================================================\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "\n",
    "def find_most_similar(word, vocab, embeddings, top_k=5):\n",
    "    \"\"\"Find most similar words\"\"\"\n",
    "    if word not in vocab:\n",
    "        return []\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        if idx != word_idx:\n",
    "            sim = cosine_similarity(word_vec, embeddings[idx])\n",
    "            similarities.append((idx, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [(idx_to_word[idx], sim) for idx, sim in similarities[:top_k]]\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"WORD SIMILARITIES\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "test_words = ['cat', 'dog', 'sat']\n",
    "for word in test_words:\n",
    "    if word in vocab:\n",
    "        print(f\"\\nMost similar to '{word}':\")\n",
    "        similar = find_most_similar(word, vocab, embeddings, top_k=3)\n",
    "        for similar_word, sim in similar:\n",
    "            print(f\"  {similar_word}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421a09ae-8463-4dd0-9cbb-3755f6846a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WORD ANALOGIES\n",
      "============================================================\n",
      "\n",
      "cat - mat + log ≈ dogs (similarity: 0.6260)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# WORD ANALOGY: king - man + woman ≈ queen\n",
    "# ============================================================\n",
    "\n",
    "def word_analogy(word_a, word_b, word_c, vocab, embeddings, idx_to_word):\n",
    "    \"\"\"\n",
    "    Solve analogy: word_a is to word_b as word_c is to ?\n",
    "    Example: king - man + woman ≈ queen\n",
    "    \"\"\"\n",
    "    if word_a not in vocab or word_b not in vocab or word_c not in vocab:\n",
    "        return None\n",
    "    \n",
    "    vec_a = embeddings[vocab[word_a]]\n",
    "    vec_b = embeddings[vocab[word_b]]\n",
    "    vec_c = embeddings[vocab[word_c]]\n",
    "    \n",
    "    # Analogy vector: (a - b) + c\n",
    "    target_vec = vec_a - vec_b + vec_c\n",
    "    \n",
    "    # Find closest word\n",
    "    best_word = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for idx in range(len(embeddings)):\n",
    "        word = idx_to_word[idx]\n",
    "        if word not in [word_a, word_b, word_c]:\n",
    "            sim = cosine_similarity(target_vec, embeddings[idx])\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_word = word\n",
    "    \n",
    "    return best_word, best_sim\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"WORD ANALOGIES\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Example: cat - mat + log ≈ dog (from our corpus)\n",
    "if all(w in vocab for w in ['cat', 'mat', 'log']):\n",
    "    result, sim = word_analogy('cat', 'mat', 'log', vocab, embeddings, idx_to_word)\n",
    "    print(f\"\\ncat - mat + log ≈ {result} (similarity: {sim:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158b812-3cfc-499a-a47d-517ae86740a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
