{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e3357b-a937-4f55-8ea8-c541e79bcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from loguru import logger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#####################\n",
    "## UNet Components ##\n",
    "#####################\n",
    "\n",
    "from ddpm_components import DoubleConv, Down, SelfAttention, Up\n",
    "\n",
    "###############\n",
    "## Utilities ##\n",
    "###############\n",
    "\n",
    "from utils import get_data, create_diffusion_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6247eb-14ca-41c6-b9ca-d2c9a122d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_dim=256, device=device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Initial Conv (no size change)\n",
    "        self.initial_conv = DoubleConv(in_channels, 24)\n",
    "        \n",
    "        # Encoder (Down) - each Down has MaxPool2d that halves spatial size\n",
    "        self.down1 = Down(24, 48)              # 128 -> 64, NO SA (too expensive)\n",
    "        self.down2 = Down(48, 96)              # 64 -> 32\n",
    "        self.sa2 = SelfAttention(96, 32)       # spatial size: 32x32\n",
    "        self.down3 = Down(96, 96)              # 32 -> 16\n",
    "        self.sa3 = SelfAttention(96, 16)       # spatial size: 16x16\n",
    "        \n",
    "        # Bottle-neck (no size change) - reduced from 256 to 192\n",
    "        self.bot1 = DoubleConv(96, 192)        # spatial size: 16x16\n",
    "        self.bot2 = DoubleConv(192, 192)       # spatial size: 16x16\n",
    "        self.bot3 = DoubleConv(192, 96)        # spatial size: 16x16\n",
    "        \n",
    "        # Decoder (Up) - each Up has Upsample that doubles spatial size\n",
    "        self.up1 = Up(192, 48)                 # 16 -> 32 (concat with x3: 96ch)\n",
    "        self.sa4 = SelfAttention(48, 32)       # spatial size: 32x32\n",
    "        self.up2 = Up(96, 24)                  # 32 -> 64 (concat with x2: 48ch), NO SA\n",
    "        self.up3 = Up(48, 24)                  # 64 -> 128 (concat with x1: 24ch), NO SA\n",
    "        \n",
    "        # Out Conv\n",
    "        self.out_conv = nn.Conv2d(24, out_channels, kernel_size=1)\n",
    "        \n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1. / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        \n",
    "        # Encoder (Down path)\n",
    "        x1 = self.initial_conv(x)      # 128x128, 24 ch\n",
    "        x2 = self.down1(x1, t)         # 64x64, 48 ch (no SA - too expensive)\n",
    "        x3 = self.down2(x2, t)         # 32x32, 96 ch\n",
    "        x3 = self.sa2(x3)              # 32x32, 96 ch\n",
    "        x4 = self.down3(x3, t)         # 16x16, 96 ch\n",
    "        x4 = self.sa3(x4)              # 16x16, 96 ch\n",
    "        \n",
    "        # Bottle-neck\n",
    "        x4 = self.bot1(x4)             # 16x16, 192 ch\n",
    "        x4 = self.bot2(x4)             # 16x16, 192 ch\n",
    "        x4 = self.bot3(x4)             # 16x16, 96 ch\n",
    "        \n",
    "        # Decoder (Up path)\n",
    "        x = self.up1(x4, x3, t)        # 32x32, 48 ch\n",
    "        x = self.sa4(x)                # 32x32, 48 ch\n",
    "        x = self.up2(x, x2, t)         # 64x64, 24 ch (no SA - too expensive)\n",
    "        x = self.up3(x, x1, t)         # 128x128, 24 ch (no SA - too expensive)\n",
    "        out = self.out_conv(x)         # 128x128, 3 ch\n",
    "        return out\n",
    "\n",
    "\n",
    "# Test the parameter count\n",
    "model = UNet().to(device=device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666178b0-3618-470d-b40c-76f0a0aabf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## Cat Images ##\n",
    "################\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "cats = load_dataset(\n",
    "    \"yashikota/cat-image-dataset\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "cats = cats.shuffle(seed=42).select(range(5000))\n",
    "cats = cats['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674137a1-9501-4fc7-8ff4-693bb2032b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## Trainning loop ##\n",
    "####################\n",
    "\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 48\n",
    "path = \"/home/longdpt/Documents/Long_AISDL/DeepLearning_PyTorch/06_DDPM\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = UNet().to(device=device)\n",
    "\n",
    "#-------\n",
    "## Train\n",
    "#-------\n",
    "\n",
    "from ddpm_components import train\n",
    "\n",
    "train(model=model, data=cats, epochs=10000, img_size=IMG_SIZE, batch_size=BATCH_SIZE, report_interval=2500, visualize=True, save_path=path+\"/cat_generator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be63ad-124b-42ed-a126-c92a4b8410b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "## Sampling ##\n",
    "##############\n",
    "\n",
    "from ddpm_components import Diffusion\n",
    "diffusion = Diffusion(img_size=IMG_SIZE, device=device)\n",
    "\n",
    "for _ in range(20):\n",
    "    _, img_list = diffusion.sample(model, n=1)\n",
    "    fig = create_diffusion_animation(img_list)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9438b30-c4b0-4763-a066-95282795ee10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
