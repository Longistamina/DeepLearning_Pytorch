{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f219d7a0-40bc-429e-b68f-01e2db7e4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from loguru import logger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724bb043-d447-4f11-b49b-d7825d045ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## Diffusion modules ##\n",
    "#######################\n",
    "\n",
    "class Diffusion:\n",
    "    '''\n",
    "    This class contains these functions:\n",
    "    + noise scheduler\n",
    "    + noising images\n",
    "    + sampling images (generate)\n",
    "    '''\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, device=device):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "        \n",
    "        self.beta = self.noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "    def noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps) # Linear scheduling\n",
    "    \n",
    "    def noise_images(self, x, t): # nois image is x_t\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None] # the resulting shape will be (batch_size, 1, 1, 1)\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1. - self.alpha_hat[t])[:, None, None, None]\n",
    "        noise = torch.randn_like(x)\n",
    "        \n",
    "        x_t = sqrt_alpha_hat*x + sqrt_one_minus_alpha_hat*noise\n",
    "        return x_t, noise\n",
    "    \n",
    "    def sample_timesteps(self, n): # create the timesteps for sampling\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "    \n",
    "    def sample(self, model, n):\n",
    "        logger.info(f\"Sampling {n} new images...\")\n",
    "        \n",
    "        _ = model.eval()\n",
    "        with torch.inference_mode():\n",
    "            x = torch.randn(size=(n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            \n",
    "            img_list = []\n",
    "            img_list.append(x)\n",
    "            \n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0, desc=\"Sampling\"):\n",
    "                t = (torch.ones(n) * i).long().to(self.device) # torch.ones create [1., 1., 1., ...], multiply i creates [i., i., i., ...], .long() for integer\n",
    "                predicted_noise = model(x, t)                  # example, at timestep i=5, with n=3 images, the resulted t=[5, 5, 5]\n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][: None, None, None]\n",
    "                beta = self.beta[t][: None, None, None]\n",
    "                \n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                    \n",
    "                x = (1/torch.sqrt(alpha)) * (x - ((1 - alpha)/torch.sqrt(1 - alpha_hat))*predicted_noise) +  torch.sqrt(beta)*noise\n",
    "                \n",
    "                img_list.append(x)\n",
    "        \n",
    "        _ = model.train()\n",
    "        \n",
    "        for idx, x in enumerate(img_list):\n",
    "            img_list[idx] = (x.clamp(-1, 1) + 1) / 2   # x.clamp(-1, 1) clips all values into [-1, 1], then (x_clamp + 1)/2 to brings back to [0, 1] range\n",
    "            x = img_list[idx] # update new values for x\n",
    "            img_list[idx] = (x * 255).type(torch.uint8) # convert to RGB pixel values (0-255)\n",
    "        \n",
    "        final_image = img_list[-1]\n",
    "        return final_image, img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b23585d-18bf-418b-8bca-9d3af3553512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## UNet components ##\n",
    "#####################\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "        \n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbab6256-c803-4b31-8483-b09db053801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## UNet architecture ##\n",
    "#######################\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_dim=256, device=device): # in_channels and out_channels are all 3 since dealing with RGB images\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Initial Conv\n",
    "        self.initial_conv = DoubleConv(in_channels, 64)\n",
    "        \n",
    "        # Encoder (Down)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128, 32)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256, 16)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256, 8)\n",
    "        \n",
    "        # Bottle-neck\n",
    "        self.bot1 = DoubleConv(256, 512)\n",
    "        self.bot2 = DoubleConv(512, 512)\n",
    "        self.bot3 = DoubleConv(512, 256)\n",
    "        \n",
    "        # Decoder (Up)\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128, 16)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64, 32)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.sa6 = SelfAttention(64, 64)\n",
    "        \n",
    "        # Out Conv\n",
    "        self.out_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1. / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float) # make t becomes column vector\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        \n",
    "        # Down\n",
    "        x1 = self.initial_conv(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "        \n",
    "        # Bottle-neck\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "        \n",
    "        # Up\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        out = self.out_conv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3e82d7d-73a8-4462-9004-38c15ac49605",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "## Utilities ##\n",
    "###############\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as pyplot\n",
    "\n",
    "def plot_images(images):\n",
    "    plt.figure(figsize=(32, 32))\n",
    "    plt.imshow( torch.cat([\n",
    "        torch.cat([img for img in images.cpu()], dim=-1)\n",
    "    ], dim=-2).permute(1, 2, 0).cpu())\n",
    "    plt.show()\n",
    "    \n",
    "def create_diffusion_animation(img_list, fps=30, skip_frames=10):\n",
    "    \"\"\"\n",
    "    Create animation of diffusion sampling process.\n",
    "    \n",
    "    Args:\n",
    "        img_list: List of image tensors from sampling\n",
    "        fps: Animation speed\n",
    "        skip_frames: Show every Nth frame (default: 10)\n",
    "    \"\"\"\n",
    "    # Take every Nth frame and first image from batch\n",
    "    frames = []\n",
    "    for i in range(0, len(img_list), skip_frames):\n",
    "        img = img_list[i][0].cpu().permute(1, 2, 0).numpy()  # [3,H,W] -> [H,W,3]\n",
    "        frames.append(img)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure(\n",
    "        data=[go.Image(z=frames[0])],\n",
    "        layout=go.Layout(\n",
    "            title=\"Diffusion Denoising Process\",\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False)\n",
    "        ),\n",
    "        frames=[go.Frame(data=[go.Image(z=frame)]) for frame in frames]\n",
    "    )\n",
    "    \n",
    "    # Add play button\n",
    "    fig.update_layout(\n",
    "        updatemenus=[{\n",
    "            \"buttons\": [\n",
    "                {\"label\": \"Play\", \"method\": \"animate\", \"args\": [None, {\"frame\": {\"duration\": 1000/fps}}]},\n",
    "                {\"label\": \"Pause\", \"method\": \"animate\", \"args\": [[None], {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\"}]}\n",
    "            ],\n",
    "            \"type\": \"buttons\"\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def save_images(images, path, **kwargs):\n",
    "    grid = torchvision.utils.make_grid(images, **kwargs)\n",
    "    ndarr = grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)\n",
    "\n",
    "def get_data(img_size, batch_size, img_list=None, path=None):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(80),\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    if (img_list is None) and (path is None):\n",
    "        logger.error(\"No data was given\")\n",
    "    \n",
    "    if path is not None:\n",
    "        dataset = torchvision.datasets.ImageFolder(path, transform=preprocess)\n",
    "    else:\n",
    "        img_transformed = torch.stack([preprocess(img) for img in img_list])\n",
    "        dataset = torch.utils.data.TensorDataset(img_transformed)\n",
    "    \n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73790ec3-ce47-4383-a9ec-2b2691e6b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## pokemon images ##\n",
    "####################\n",
    "\n",
    "from datasets import load_dataset\n",
    "pokemon = load_dataset(path=\"reach-vb/pokemon-blip-captions\", split=\"train\")\n",
    "pokemon = pokemon['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16260d74-d090-4a7c-a536-7d6eada5487c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5c667b2881408ba75fdd45e63c6fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [SelfAttention] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     61\u001b[39m             figh.show()\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m#-------\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m## Train\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m#-------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epochs)\u001b[39m\n\u001b[32m     31\u001b[39m t = diffusion.sample_timesteps(images.shape[\u001b[32m0\u001b[39m]).to(device)\n\u001b[32m     32\u001b[39m x_t, noise = diffusion.noise_images(images, t)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m predicted_noise = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m loss = loss_fn(noise, predicted_noise)\n\u001b[32m     36\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mUNet.forward\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m     53\u001b[39m x1 = \u001b[38;5;28mself\u001b[39m.initial_conv(x)\n\u001b[32m     54\u001b[39m x2 = \u001b[38;5;28mself\u001b[39m.down1(x1, t)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m x2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msa1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m x3 = \u001b[38;5;28mself\u001b[39m.down2(x2, t)\n\u001b[32m     57\u001b[39m x3 = \u001b[38;5;28mself\u001b[39m.sa2(x3)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/module.py:399\u001b[39m, in \u001b[36m_forward_unimplemented\u001b[39m\u001b[34m(self, *input)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, *\u001b[38;5;28minput\u001b[39m: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    389\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m    391\u001b[39m \u001b[33;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m \u001b[33;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    400\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] is missing the required \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforward\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m function\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    401\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Module [SelfAttention] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## Trainning loop ##\n",
    "####################\n",
    "\n",
    "IMG_SIZE = 64\n",
    "\n",
    "def train(epochs):\n",
    "    dataloader = get_data(img_size=IMG_SIZE, batch_size=32, img_list=pokemon)\n",
    "    model = UNet().to(device=device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=5, factor=0.5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    diffusion = Diffusion(img_size=IMG_SIZE, device=device)\n",
    "    l = len(dataloader)\n",
    "    \n",
    "    max_patience = 10\n",
    "    best_loss = float('inf')\n",
    "    patience_count = 0\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1), desc=\"Training\"):\n",
    "        \n",
    "        epoch_loss = 0 # Track loss to give the scheduler a meaningful average\n",
    "        for i, batch in enumerate(dataloader):\n",
    "        # Handle both ImageFolder (returns tuple) and TensorDataset (returns tuple)\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                images = batch[0]\n",
    "            else:\n",
    "                images = batch\n",
    "\n",
    "            images = images.to(device)\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            predicted_noise = model(x_t, t)\n",
    "            loss = loss_fn(noise, predicted_noise)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Early stop\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patient_count = 0\n",
    "        else:\n",
    "            patience_count += 1\n",
    "        if patience_count > max_patience:\n",
    "            break\n",
    "        \n",
    "        if (epoch == 1) or (epoch % 10 == 0):\n",
    "            print(\"+\"*50)\n",
    "            print(f\"Epoch: {epoch} | Loss: {avg_loss} | Current LR: {current_lr}\")\n",
    "            _, img_list = diffusion.sample(model, n=1)\n",
    "            fig = create_diffusion_animation(img_list)\n",
    "            figh.show()\n",
    "\n",
    "#-------\n",
    "## Train\n",
    "#-------\n",
    "\n",
    "train(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0b93d-26a2-4924-8fb3-c2c557b20cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
