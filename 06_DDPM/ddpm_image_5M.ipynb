{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e3357b-a937-4f55-8ea8-c541e79bcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from loguru import logger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "###############\n",
    "## Utilities ##\n",
    "###############\n",
    "\n",
    "from utils import get_data, create_diffusion_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f6247eb-14ca-41c6-b9ca-d2c9a122d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5897155\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "## UNet architecture ##\n",
    "#######################\n",
    "\n",
    "from ddpm_components import DoubleConv, Down, SelfAttention, Up\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_dim=256, device=device): # in_channels and out_channels are all 3 since dealing with RGB images\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Initial Conv\n",
    "        self.initial_conv = DoubleConv(in_channels, 32)\n",
    "        \n",
    "        # Encoder (Down)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.sa1 = SelfAttention(64, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.sa2 = SelfAttention(128, 32)\n",
    "        self.down3 = Down(128, 128)\n",
    "        self.sa3 = SelfAttention(128, 8)\n",
    "        \n",
    "        # Bottle-neck\n",
    "        self.bot1 = DoubleConv(128, 256)\n",
    "        self.bot2 = DoubleConv(256, 256)\n",
    "        self.bot3 = DoubleConv(256, 128)\n",
    "        \n",
    "        # Decoder (Up)\n",
    "        self.up1 = Up(256, 64)\n",
    "        self.sa4 = SelfAttention(64, 32)\n",
    "        self.up2 = Up(128, 32)\n",
    "        self.sa5 = SelfAttention(32, 64)\n",
    "        self.up3 = Up(64, 32)\n",
    "        self.sa6 = SelfAttention(32, 32)\n",
    "        \n",
    "        # Out Conv\n",
    "        self.out_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        \n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1. / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float) # make t becomes column vector\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        \n",
    "        # Down\n",
    "        x1 = self.initial_conv(x)\n",
    "        x2 = self.down1(x1, t)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t)\n",
    "        x4 = self.sa3(x4)\n",
    "        \n",
    "        # Bottle-neck\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "        \n",
    "        # Up\n",
    "        x = self.up1(x4, x3, t)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t)\n",
    "        x = self.sa6(x)\n",
    "        out = self.out_conv(x)\n",
    "        return out\n",
    "\n",
    "model = UNet()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666178b0-3618-470d-b40c-76f0a0aabf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "## Flower Images ##\n",
    "###################\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from datasets import load_dataset\n",
    "flowers = load_dataset(path=\"nkirschi/oxford-flowers\", split=\"test\")\n",
    "flowers = flowers['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "674137a1-9501-4fc7-8ff4-693bb2032b32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#-------\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m## Train\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#-------\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mddpm_components\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflowers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Long_AISDL/DeepLearning_PyTorch/06_DDPM/ddpm_components.py:261\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, data, epochs, lr, img_size, batch_size, visualize, report_interval)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(model, data, epochs, lr=\u001b[32m2e-4\u001b[39m, img_size=IMG_SIZE, batch_size=BATCH_SIZE, visualize=\u001b[38;5;28;01mFalse\u001b[39;00m, report_interval=\u001b[32m1000\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m     dataloader = \u001b[43mget_data\u001b[49m(img_size=img_size, batch_size=batch_size, img_list=data)\n\u001b[32m    262\u001b[39m     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\u001b[32m    263\u001b[39m     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m, patience=\u001b[32m250\u001b[39m, factor=\u001b[32m0.8\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'get_data' is not defined"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## Trainning loop ##\n",
    "####################\n",
    "\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = UNet().to(device=device)\n",
    "\n",
    "#-------\n",
    "## Train\n",
    "#-------\n",
    "\n",
    "from ddpm_components import train\n",
    "\n",
    "train(model=model, data=flowers, epochs=4000, img_size=IMG_SIZE, batch_size=BATCH_SIZE, report_interval=1000, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579e4ad-16db-4bcb-9504-91477eaaf87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
