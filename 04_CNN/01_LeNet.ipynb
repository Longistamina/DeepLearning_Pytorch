{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c06dba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Importing libraries ##\n",
    "#########################\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abd5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebe097862ef4971b9160358c14c6f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/120M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989f1c4108af4d1497e4df74204378ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/23.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3c87b4f97b482f9c75b69bde74fd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b102245b1e90482d93db84294f31f0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['img', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Dataset downloading ##\n",
    "#########################\n",
    "\n",
    "# Import CIFAR-10 dataset from HuggingFace\n",
    "# Run this first in terminal: pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_set = load_dataset(\n",
    "    'cifar10',\n",
    "    split='train', # Download the training set\n",
    "    verification_mode='basic_checks'  # checks if the data files exist and verifies basic metadata\n",
    ")\n",
    "\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f3180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['img', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "val_set =  load_dataset(\n",
    "    'cifar10',\n",
    "    split='test', # Download the training set\n",
    "    verification_mode='basic_checks'  # checks if the data files exist and verifies basic metadata\n",
    ")\n",
    "\n",
    "print(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b11699f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDeXxDYm8MHmfKB/rD93PpUsGt2UyufPRdpbAJwSB3rk544YA5cHkhAzLx/9aoAGXAQrn+EOM5FXcDsI9cs3BZpUVd20HPt1NQrrlrcX32SLc+QT5gHy1y5ljcZllUndt35rPeWaF2iDM6Y52jGc0XAt67ezWz+W6J5EjbhID3HbHrQlrqLW8c8MRkikHytCpf88dKimEVyNsyRsByDvIOfWizkuNKlkk0+/miaT7yvIWVj9DSvGw7GrD4c1CWMyfOpbBIaIjmpJPDN5wZZiqY67OlUpdb1W5t2iu79GyB9wbefzqODWb60iWJb1mj7rntU3QWP/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an image\n",
    "train_set[0]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4478676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7A2EBA2F0410>\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0]['img'])\n",
    "# <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7A2EBA2F0410>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf76b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250aa6e69d9b411b82c4a6fef43280db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Images:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################\n",
    "## Image preprocessing ##\n",
    "#########################\n",
    "\n",
    "IMG_SIZE = 32\n",
    "'''\n",
    "Most CNNs are designed to only accept images of a fixed size\n",
    "=> Must fix the IMG_SIZE, and reshape the input to adapt this norm.\n",
    "'''\n",
    "\n",
    "#----\n",
    "## Build preprocess transforms\n",
    "#----\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), # Resize the input image to a given size (IMG_SIZE, IMG_SIZE)\n",
    "        transforms.ToTensor()                  # Convert to tensor (and also convert to [0, 1] tensors)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#----\n",
    "## Change from grayscale to RGB, and apply preprocess\n",
    "#----\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "inputs_train = []\n",
    "\n",
    "for record in tqdm(iterable=train_set, desc=\"Preprocessing Images\"):\n",
    "    image = record['img']\n",
    "    label = record['label']\n",
    "    \n",
    "    # Convert from grayscale to RGB (3-colour channels)\n",
    "    if image.mode == \"L\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # preprocession\n",
    "    input_tensor = preprocess(image)\n",
    "    \n",
    "    # append to inputs_train\n",
    "    inputs_train.append([input_tensor, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16384, 32])\n",
      "tensor([0.4855, 0.4792, 0.4421])\n",
      "tensor([0.2464, 0.2418, 0.2599])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdbb85c7dd94dd59b49b59a16e346eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----\n",
    "## Re-normalize the pixel values for train set\n",
    "#----\n",
    "'''\n",
    "Since transforms.Tensor() normalizes all into [0, 1],\n",
    "we need to modify this normalization to fit this dataset.\n",
    "\n",
    "Doing so by calculating the mean and std for all images across separe 3 color channles\n",
    "then use transforms.Normalize(mean=, std=) with this calculated mean and std.\n",
    "'''\n",
    "\n",
    "# First, we need to calculate the mean and std for each of the RGB channels across all images\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Choosing a random sample to calculate mean and std (this sample containing random 512 images)\n",
    "np.random.seed(0)\n",
    "idx = np.random.randint(0, len(inputs_train), 512)\n",
    "\n",
    "# Concatenate this subset of images into a new tensor )tensor_placeholder)\n",
    "tensor_placeholder = torch.concat([inputs_train[i][0] for i in idx], axis=1)\n",
    "print(tensor_placeholder.shape)\n",
    "# torch.Size([3, 16384, 32])\n",
    "'''\n",
    "we concatenate 512 images of size (3x32x32) (Channel*Height*Width) along the Height channel\n",
    "=> (3x16384x32), 16384=32*512\n",
    "'''\n",
    "\n",
    "# Calculate the mean and std across all images, for separate channel\n",
    "mean_all = torch.mean(tensor_placeholder, dim=(1, 2)) # dim=(1, 2) meanin only uses Heigh*Width for calculation, ignore the channel\n",
    "std_all = torch.std(tensor_placeholder, dim=(1, 2))\n",
    "\n",
    "print(mean_all) # tensor([0.4855, 0.4792, 0.4421])\n",
    "print(std_all) # tensor([0.2464, 0.2418, 0.2599])\n",
    "\n",
    "#### RE-NORMALIZE ###\n",
    "\n",
    "preprocess = transforms.Compose([transforms.Normalize(mean=mean_all, std=std_all)])\n",
    "\n",
    "for idx in tqdm(range(len(inputs_train))):\n",
    "    input_tensor = preprocess(inputs_train[idx][0])\n",
    "    inputs_train[idx][0] = input_tensor # replace with re-normalized tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca8d77a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54454d68f31241f08c75da8032ed280b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Images:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#----\n",
    "## Re-normalize the pixel values for val set\n",
    "#----\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), # Resize the input image to a given size (IMG_SIZE, IMG_SIZE)\n",
    "        transforms.ToTensor(),                  # Convert to tensor (and also convert to [0, 1] tensors)\n",
    "        transforms.Normalize(mean=mean_all, std=std_all) # Re-normalize with new mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "inputs_val = []\n",
    "\n",
    "for record in tqdm(iterable=val_set, desc=\"Preprocessing Images\"):\n",
    "    image = record['img']\n",
    "    label = record['label']\n",
    "    \n",
    "    # Convert from grayscale to RGB (3-colour channels)\n",
    "    if image.mode == \"L\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # preprocession\n",
    "    input_tensor = preprocess(image)\n",
    "    \n",
    "    # append to inputs_train\n",
    "    inputs_val.append([input_tensor, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07b7a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## Dataloader ##\n",
    "################\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(inputs_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(inputs_val, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a55048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
