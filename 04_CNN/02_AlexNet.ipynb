{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AlexNet architecture\n",
    "+---------+-----------------+-------------------------------+------------------+\n",
    "| Layer   | Type            | Configuration                 | Output Size      |\n",
    "+---------+-----------------+-------------------------------+------------------+\n",
    "| Input   | Image           | 227 x 227 x 3 (RGB)           | 227 x 227 x 3    |\n",
    "| Conv 1  | Convolution     | 96 filters (11x11), Stride 4  | 55 x 55 x 96     |\n",
    "| Pool 1  | Max Pooling     | 3x3 window, Stride 2          | 27 x 27 x 96     |\n",
    "| Conv 2  | Convolution     | 256 filters (5x5), Padding 2  | 27 x 27 x 256    |\n",
    "| Pool 2  | Max Pooling     | 3x3 window, Stride 2          | 13 x 13 x 256    |\n",
    "| Conv 3  | Convolution     | 384 filters (3x3), Padding 1  | 13 x 13 x 384    |\n",
    "| Conv 4  | Convolution     | 384 filters (3x3), Padding 1  | 13 x 13 x 384    |\n",
    "| Conv 5  | Convolution     | 256 filters (3x3), Padding 1  | 13 x 13 x 256    |\n",
    "| Pool 3  | Max Pooling     | 3x3 window, Stride 2          | 6 x 6 x 256      |\n",
    "| FC 6    | Fully Connected | 4096 Neurons + Dropout        | 4096             |\n",
    "| FC 7    | Fully Connected | 4096 Neurons + Dropout        | 4096             |\n",
    "| FC 8    | Fully Connected | 1000 Neurons (Softmax)        | 1000             |\n",
    "+---------+-----------------+-------------------------------+------------------+\n",
    "\n",
    "After the Conv3 and Conv4, we don't add MaxPool or AvgPool to avoid shrinking too quickly,\n",
    "losing critical spatial information before the network could extract high-level features.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06dba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Importing libraries ##\n",
    "#########################\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from alive_progress import alive_bar, alive_it\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4abd5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['img', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "## Dataset downloading ##\n",
    "#########################\n",
    "'''\n",
    "Import CIFAR-10 dataset from HuggingFace\n",
    "Run this first in terminal: pip install datasets\n",
    "'''\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_set = load_dataset(\n",
    "    'cifar10',\n",
    "    split='train', # Download the training set\n",
    "    verification_mode='basic_checks'  # checks if the data files exist and verifies basic metadata\n",
    ")\n",
    "\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f3180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['img', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "val_set =  load_dataset(\n",
    "    'cifar10',\n",
    "    split='test', # Download the training set\n",
    "    verification_mode='basic_checks'  # checks if the data files exist and verifies basic metadata\n",
    ")\n",
    "\n",
    "print(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b11699f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDeXxDYm8MHmfKB/rD93PpUsGt2UyufPRdpbAJwSB3rk544YA5cHkhAzLx/9aoAGXAQrn+EOM5FXcDsI9cs3BZpUVd20HPt1NQrrlrcX32SLc+QT5gHy1y5ljcZllUndt35rPeWaF2iDM6Y52jGc0XAt67ezWz+W6J5EjbhID3HbHrQlrqLW8c8MRkikHytCpf88dKimEVyNsyRsByDvIOfWizkuNKlkk0+/miaT7yvIWVj9DSvGw7GrD4c1CWMyfOpbBIaIjmpJPDN5wZZiqY67OlUpdb1W5t2iu79GyB9wbefzqODWb60iWJb1mj7rntU3QWP/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View an image\n",
    "train_set[0]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4478676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7D0995981AF0>\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0]['img'])\n",
    "# <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x7A2EBA2F0410>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7bf76b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baf0cd4debc425f86e9daa3180c23ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Images:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#########################\n",
    "## Image preprocessing ##\n",
    "#########################\n",
    "\n",
    "IMG_SIZE = 32\n",
    "'''\n",
    "Most CNNs are designed to only accept images of a fixed size\n",
    "=> Must fix the IMG_SIZE, and reshape the input to adapt this norm.\n",
    "'''\n",
    "\n",
    "#----\n",
    "## Build preprocess transforms\n",
    "#----\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), # Resize the input image to a given size (IMG_SIZE, IMG_SIZE)\n",
    "        transforms.ToTensor()                  # Convert to tensor (and also convert to [0, 1] tensors)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#----\n",
    "## Change from grayscale to RGB, and apply preprocess\n",
    "#----\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "inputs_train = []\n",
    "\n",
    "for record in tqdm(iterable=train_set, desc=\"Preprocessing Images\"):\n",
    "    image = record['img']\n",
    "    label = record['label']\n",
    "    \n",
    "    # Convert from grayscale to RGB (3-colour channels)\n",
    "    if image.mode == \"L\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # preprocessing\n",
    "    input_tensor = preprocess(image)\n",
    "    label_tensor = torch.tensor(label)\n",
    "    \n",
    "    # append to inputs_train\n",
    "    inputs_train.append([input_tensor, torch.tensor(label)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e5b7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16384, 32])\n",
      "tensor([0.4855, 0.4792, 0.4421])\n",
      "tensor([0.2464, 0.2418, 0.2599])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7367255e00e41bc9aeede2dcca2b89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----\n",
    "## Re-normalize the pixel values for train set\n",
    "#----\n",
    "'''\n",
    "Since transforms.Tensor() normalizes all into [0, 1],\n",
    "we need to modify this normalization to fit this dataset.\n",
    "\n",
    "Doing so by calculating the mean and std for all images across separe 3 color channles\n",
    "then use transforms.Normalize(mean=, std=) with this calculated mean and std.\n",
    "'''\n",
    "\n",
    "# First, we need to calculate the mean and std for each of the RGB channels across all images\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Choosing a random sample to calculate mean and std (this sample containing random 512 images)\n",
    "np.random.seed(0)\n",
    "idx = np.random.randint(0, len(inputs_train), 512)\n",
    "\n",
    "# Concatenate this subset of images into a new tensor )tensor_placeholder)\n",
    "tensor_placeholder = torch.concat([inputs_train[i][0] for i in idx], axis=1)\n",
    "print(tensor_placeholder.shape)\n",
    "# torch.Size([3, 16384, 32])\n",
    "'''\n",
    "we concatenate 512 images of size (3x32x32) (Channel*Height*Width) along the Height channel\n",
    "=> (3x16384x32), 16384=32*512\n",
    "'''\n",
    "\n",
    "# Calculate the mean and std across all images, for separate channel\n",
    "mean_all = torch.mean(tensor_placeholder, dim=(1, 2)) # dim=(1, 2) meanin only uses Heigh*Width for calculation, ignore the channel\n",
    "std_all = torch.std(tensor_placeholder, dim=(1, 2))\n",
    "\n",
    "print(mean_all) # tensor([0.4855, 0.4792, 0.4421])\n",
    "print(std_all) # tensor([0.2464, 0.2418, 0.2599])\n",
    "\n",
    "#### RE-NORMALIZE ###\n",
    "\n",
    "preprocess = transforms.Compose([transforms.Normalize(mean=mean_all, std=std_all)])\n",
    "\n",
    "for idx in tqdm(range(len(inputs_train))):\n",
    "    input_tensor = preprocess(inputs_train[idx][0])\n",
    "    inputs_train[idx][0] = input_tensor # replace with re-normalized tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca8d77a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cc28b012654e1b864903536449ce60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Images:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#----\n",
    "## Re-normalize the pixel values for val set\n",
    "#----\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), # Resize the input image to a given size (IMG_SIZE, IMG_SIZE)\n",
    "        transforms.ToTensor(),                  # Convert to tensor (and also convert to [0, 1] tensors)\n",
    "        transforms.Normalize(mean=mean_all, std=std_all) # Re-normalize with new mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "inputs_val = []\n",
    "\n",
    "for record in tqdm(iterable=val_set, desc=\"Preprocessing Images\"):\n",
    "    image = record['img']\n",
    "    label = record['label']\n",
    "    \n",
    "    # Convert from grayscale to RGB (3-colour channels)\n",
    "    if image.mode == \"L\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    # preprocessing\n",
    "    input_tensor = preprocess(image)\n",
    "    label_tensor = torch.tensor(label)\n",
    "    \n",
    "    # append to inputs_train\n",
    "    inputs_val.append([input_tensor, torch.tensor(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07b7a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## Dataloader ##\n",
    "################\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(inputs_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(inputs_val, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9a55048",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "## Building AlexNet-inspired CNN ##\n",
    "###################################\n",
    "'''\n",
    "At each block, the images are downsampled by the max-pooling layer. \n",
    "\n",
    "Contrary, the number of channels from one layer to another increased from 3 to 64, to 192, ... to 256. \n",
    "=> As we learned before, deeper layers have larger receptive fields and generally detect more specific and complex features, \n",
    "such as ears, eyes, or even human faces and dogs. The chosen filter (or kernel) size is either or even human faces and dogs.\n",
    "\n",
    "How will it increase from 3 channels to 64 channels?\n",
    "    => The layer creates 64 separate filters (kernels), where each filter processes all 3 input channels together\n",
    "    => Each has shape: 4x4x3 (kernel_size=4, and depth=3 to match input channels)\n",
    "    => Filter 1 (4x4x3) convolves with RGB input → produces feature map 1\n",
    "       Filter 2 (4x4x3) convolves with RGB input → produces feature map 2\n",
    "       ...\n",
    "       Filter 64 (4x4x3) convolves with RGB input → produces feature map 64\n",
    "    => Stack all 64 feature maps together = 64 output channels\n",
    "\n",
    "The kernel_size refers to the height and width of the sliding window (also called filter)\n",
    "The chosen filter (or kernel) size is either 3 or 4. Example, kernel_size=4 => sliding window is 4x4\n",
    "This is a common choice - having a smaller filter allows the network to better generalize. \n",
    "\n",
    "Padding is the process of adding a \"border\" of extra pixels (usually zeros) around the edges of your input image before the convolution operation begins.\n",
    "Padding helps avoid shrinkage and loss of edge information.\n",
    "=> Here, padding is 1 pixel on each layer.\n",
    "'''\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Block 1: conv -> relu -> max_pool\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Block 2: conv -> relu -> max_pool\n",
    "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Block 3: conv -> relu\n",
    "            nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 4: conv -> relu\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "                       \n",
    "            # Block 5: conv -> relu -> max_pool\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            # Convolutional and pooling layers output 4D tensors (Batch, Channels, Height, Width)\n",
    "            # However, Fully Connected layers expect 2D tensors (Batch, Features)\n",
    "            # => must FLATTEN the 4D tensor into a 2D tensor\n",
    "            nn.Flatten(),\n",
    "                        \n",
    "            # Block 6: drop_out -> fc_linear -> relu\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.LazyLinear(512), # nn.Linear() requires input_size (1024) and output_size, using nn.LazyLinear() will automaticall calculate the input_size for us\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 7: drop_out -> fc_linear -> relu\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.LazyLinear(256), # nn.Linear() requires input_size (1024) and output_size, using nn.LazyLinear() will automaticall calculate the input_size for us\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            \n",
    "            # Block 8: fc_linear -> final logits (output)\n",
    "            nn.LazyLinear(num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.cnn(X)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ae6c667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "## model initialization ##\n",
    "##########################\n",
    "\n",
    "num_classes = len(set(train_set['label']))\n",
    "print(num_classes) # 10\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = AlexNet(num_classes=num_classes).to(device)\n",
    "\n",
    "##################################\n",
    "## Loss - Optimizer - Scheduler ##\n",
    "##################################\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=5, factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff22bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d5ccd90ed64748bd394804d17c3df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 10\n",
      "Train loss: 2.2106\n",
      "Validation loss: 2.0176\n",
      "Current LR: 0.0050\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 20\n",
      "Train loss: 2.2526\n",
      "Validation loss: 2.2554\n",
      "Current LR: 0.0010\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 30\n",
      "Train loss: 2.1977\n",
      "Validation loss: 2.1842\n",
      "Current LR: 0.0000\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 40\n",
      "Train loss: 2.3100\n",
      "Validation loss: 2.1797\n",
      "Current LR: 0.0000\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 50\n",
      "Train loss: 2.2538\n",
      "Validation loss: 2.1797\n",
      "Current LR: 0.0000\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 60\n",
      "Train loss: 2.1119\n",
      "Validation loss: 2.1797\n",
      "Current LR: 0.0000\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 70\n",
      "Train loss: 2.0938\n",
      "Validation loss: 2.1797\n",
      "Current LR: 0.0000\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 80\n",
      "Train loss: 2.2248\n",
      "Validation loss: 2.1797\n",
      "Current LR: 0.0000\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 90\n",
      "Train loss: 2.3107\n",
      "Validation loss: 2.1797\n",
      "Current LR: 0.0000\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 100\n",
      "Train loss: 2.1964\n",
      "Validation loss: 2.1797\n",
      "Current LR: 0.0000\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "## Training - Validating loop ##\n",
    "################################\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "for epoch in tqdm(iterable=range(1, epochs+1, 1), desc=\"Training\"):\n",
    "    # --- TRAINING ---\n",
    "    _ = model.train() # Turn on training mode, enable gradient tracking\n",
    "    for _, (images, labels) in enumerate(train_loader):\n",
    "        # moves values to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # (Standard training steps: forward, loss, zero_grad, backward, step)\n",
    "        preds = model(images).squeeze()\n",
    "        loss = loss_fn(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # --- VALIDATION (Every epoch) ---\n",
    "    _ = model.eval() # 1. Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.inference_mode(): # 2. Turn off gradient tracking to save memory\n",
    "        for _, (images, labels) in enumerate(val_loader): # 3. Iterate through val_set\n",
    "            # moves values to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # calculate predictions\n",
    "            val_preds = model(images).squeeze()\n",
    "            \n",
    "            # Accumulate loss to get an average for the whole set\n",
    "            val_loss += loss_fn(val_preds, labels).item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            \n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    train_loss_list.append(loss.item())\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"+\"*50)\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(f\"Train loss: {loss:.4f}\")\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Current LR: {current_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5f3ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
